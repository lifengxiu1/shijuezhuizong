import cv2
import numpy as np
import time
import mediapipe as mp

# ====== æ–°å¢ï¼šSO100 æœºå™¨äººç›¸å…³ï¼ˆé€‚é… lerobot 0.4.1 çš„è·¯å¾„ï¼‰ ======
from lerobot.robots.so100_follower.so100_follower import SO100Follower
from lerobot.robots.so100_follower.config_so100_follower import SO100FollowerConfig


# ---------- ç»˜åˆ¶è¾…åŠ© ----------
def draw_gesture_status(img, is_tap, is_hold):
    h, w, _ = img.shape
    if is_tap:
        text, color = "TAP!", (0, 255, 255)      # é»„
    elif is_hold:
        text, color = "HOLD!", (255, 0, 255)     # æ´‹çº¢
    else:
        text, color = "Watching...", (255, 255, 255)
    cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)

    tips = [
        "Pinch index & thumb:",
        "Quick pinch = TAP (yellow)",
        "Long pinch = HOLD (magenta)",
        "Press 'q' to quit",
    ]
    for i, t in enumerate(tips):
        cv2.putText(img, t, (10, h - 100 + i * 25),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)


def draw_hand(img, palm_center, is_tap=False, is_hold=False):
    h, w, _ = img.shape
    # ä¼ è¿›æ¥æ˜¯[-1,1]åæ ‡ï¼Œè¿™é‡Œè½¬åƒç´ å¹¶è€ƒè™‘é•œåƒ
    px = int(w - ((-palm_center[0] + 1) / 2) * w)
    py = int(((palm_center[1] + 1) / 2) * h)
    color = (0, 0, 255)
    if is_tap:
        color = (0, 255, 255)
    elif is_hold:
        color = (255, 0, 255)
    cv2.circle(img, (px, py), 8, color, -1)


def draw_finger_tips(img, tracker, is_hold=False):
    if tracker.index_pos is None or tracker.thumb_pos is None:
        return
    h, w, _ = img.shape
    ix = int(w - tracker.index_pos[0] * w)
    iy = int(tracker.index_pos[1] * h)
    tx = int(w - tracker.thumb_pos[0] * w)
    ty = int(tracker.thumb_pos[1] * h)
    index_px, thumb_px = (ix, iy), (tx, ty)

    cv2.circle(img, index_px, 8, (0, 255, 0), -1)     # é£ŸæŒ‡
    cv2.circle(img, thumb_px, 8, (255, 255, 0), -1)   # æ‹‡æŒ‡
    cv2.line(img, index_px, thumb_px, (255, 255, 255), 2)

    mid_x, mid_y = (ix + tx) // 2, (iy + ty) // 2
    if tracker.current_distance is not None:
        cv2.putText(img, f"Distance: {tracker.current_distance:.3f}",
                    (mid_x - 50, mid_y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                    (255, 255, 255), 1)
        cv2.putText(img, f"Duration: {tracker.current_duration:.2f}s",
                    (mid_x - 50, mid_y + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                    (255, 255, 255), 1)

    cv2.putText(img, "INDEX", (ix - 30, iy - 15),
                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
    cv2.putText(img, "THUMB", (tx - 30, ty - 15),
                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)

    if is_hold:
        center = (w // 2, h // 2)
        cv2.line(img, center, (mid_x, mid_y), (255, 0, 0), 3)
        cv2.circle(img, center, 5, (255, 0, 0), -1)
        cv2.circle(img, (mid_x, mid_y), 5, (255, 0, 0), -1)
        err = np.hypot(mid_x - center[0], mid_y - center[1])
        cv2.putText(img, f"Error: {err:.1f}px", (center[0] + 10, center[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)


# ---------- æ‰‹åŠ¿é€»è¾‘ ----------
mp_hands = mp.solutions.hands


class HandTracker:
    def __init__(self, nb_hands=1, tap_threshold=0.25, hold_threshold=0.6, distance_threshold=0.05):
        self.hands = mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=nb_hands,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.tap_threshold = tap_threshold
        self.hold_threshold = hold_threshold
        self.distance_threshold = distance_threshold
        self.finger_down_start = None
        self.is_finger_down = False

        self.current_distance = None
        self.current_duration = 0.0
        self.index_pos = None
        self.thumb_pos = None
        self.just_tapped = False

    def get_palm_centers(self, img):
        img = cv2.flip(img, 1)
        res = self.hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        if not res.multi_hand_landmarks or not res.multi_handedness:
            return None
        centers = []
        for lms, handed in zip(res.multi_hand_landmarks, res.multi_handedness):
            if handed.classification[0].label != 'Right':
                continue
            pip = lms.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_PIP]
            centers.append([-(pip.x - 0.5) * 2, (pip.y - 0.5) * 2])
        return centers or None

    def update(self, img):
        img = cv2.flip(img, 1)
        res = self.hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        now = time.time()
        self.just_tapped = False

        if res.multi_hand_landmarks and res.multi_handedness:
            for lms, handed in zip(res.multi_hand_landmarks, res.multi_handedness):
                if handed.classification[0].label != 'Right':
                    continue
                idx = lms.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]
                thb = lms.landmark[mp_hands.HandLandmark.THUMB_TIP]
                self.index_pos = np.array([idx.x, idx.y])
                self.thumb_pos = np.array([thb.x, thb.y])
                self.current_distance = float(np.linalg.norm(self.index_pos - self.thumb_pos))

                if not self.is_finger_down and self.current_distance < self.distance_threshold:
                    self.is_finger_down = True
                    self.finger_down_start = now

                if self.is_finger_down:
                    self.current_duration = (now - self.finger_down_start) if self.finger_down_start else 0.0
                    if self.current_distance > self.distance_threshold:
                        if self.current_duration < self.tap_threshold:
                            self.just_tapped = True
                        self._reset()
                else:
                    self.current_duration = 0.0
                return  # åªå–ç¬¬ä¸€åªå³æ‰‹

        self._reset()
        self.current_distance = None
        self.index_pos = None
        self.thumb_pos = None

    def isTap(self):
        return self.just_tapped

    def isHold(self):
        return (
            self.is_finger_down and
            self.current_distance is not None and
            self.current_distance < self.distance_threshold and
            self.current_duration >= self.hold_threshold
        )

    def _reset(self):
        self.finger_down_start = None
        self.is_finger_down = False
        self.current_duration = 0.0


# ====== SO100 ç›¸å…³é…ç½® ======
JOINT_NAMES = [
    "shoulder_pan.pos",
    "shoulder_lift.pos",
    "elbow_flex.pos",
    "wrist_flex.pos",
    "wrist_roll.pos",
    "gripper.pos",
]


def init_robot():
    """
    åˆå§‹åŒ– SO100 ä»è‡‚ï¼š
    - ä½¿ç”¨ç«¯å£ COM3ï¼ˆä½ å‰é¢æ ¡å‡†ç”¨çš„é‚£ä¸ªï¼‰
    - id ç•™ç©ºï¼ˆNoneï¼‰ï¼Œå¯¹åº”åˆšåˆšç”Ÿæˆçš„ None.json æ ¡å‡†æ–‡ä»¶
    """
    cfg = SO100FollowerConfig(
        port="COM3",   # å¦‚ä¸²å£å˜äº†ï¼Œè¿™é‡Œæ”¹
        # å…¶ä»–å­—æ®µä½¿ç”¨é»˜è®¤å€¼ï¼šid=None, cameras={}, use_degrees=False ç­‰
    )
    robot = SO100Follower(cfg)
    # æ ¡å‡†å·²ç»é€šè¿‡ lerobot-calibrate åšè¿‡äº†ï¼Œè¿™é‡Œåªéœ€è¦ connect
    robot.connect()
    print("âœ… SO100 Follower å·²è¿æ¥ (COM3)")
    return robot


# ---------- ä¸»å¾ªç¯ ----------
def main():
    # å…ˆè¿ä¸Šæœºæ¢°è‡‚
    robot = init_robot()

    CAM_INDEX = 1      # å¦‚æœä½ çš„ USB æ‘„åƒå¤´æ˜¯ 1/2ï¼Œå°±æ”¹æˆ 1/2
    WIDTH, HEIGHT = 1280, 720

    cap = cv2.VideoCapture(CAM_INDEX)
    if not cap.isOpened():
        print("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        robot.disconnect()
        return

    cap.set(cv2.CAP_PROP_FRAME_WIDTH, WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HEIGHT)
    cap.set(cv2.CAP_PROP_FPS, 30)

    tracker = HandTracker(
        nb_hands=1,
        tap_threshold=0.5,
        hold_threshold=0.6,
        distance_threshold=0.05
    )

    while True:
        ok, frame = cap.read()
        if not ok:
            break

        tracker.update(frame)
        is_tap = tracker.isTap()
        is_hold = tracker.isHold()

        # ---------- è¯»å–å½“å‰å…³èŠ‚çŠ¶æ€ ----------
        try:
            obs = robot.get_observation()
            current = np.array([obs[name] for name in JOINT_NAMES], dtype=np.float32)
        except Exception as e:
            print("è¯»å–æœºæ¢°è‡‚çŠ¶æ€å¤±è´¥ï¼š", e)
            current = None

        # ---------- ç”¨â€œæä½â€æ‰‹åŠ¿æ‹–åŠ¨æœºæ¢°è‡‚ ----------
        if (
            current is not None
            and is_hold
            and tracker.index_pos is not None
            and tracker.thumb_pos is not None
        ):
            h, w, _ = frame.shape

            # æ‰‹æŒ‡ä¸­ç‚¹ï¼ˆ0~1 åæ ‡ï¼‰
            mid_x = (tracker.index_pos[0] + tracker.thumb_pos[0]) / 2.0
            mid_y = (tracker.index_pos[1] + tracker.thumb_pos[1]) / 2.0

            # æ˜ å°„åˆ°åƒç´ åæ ‡ï¼ˆæŒ‰ä½ ä¹‹å‰çš„åæ ‡ç³»åšå·¦å³é•œåƒï¼‰
            px = int((1.0 - mid_x) * w)
            py = int(mid_y * h)

            # ç›¸å¯¹å›¾åƒä¸­å¿ƒçš„åç§»ï¼ŒèŒƒå›´å¤§è‡´ [-1, 1]
            cx, cy = w // 2, h // 2
            off_x = (px - cx) / cx      # å³æ­£
            off_y = (py - cy) / cy      # ä¸‹æ­£

            # å°†åç§»æ˜ å°„æˆå…³èŠ‚çš„å°å¢é‡ï¼ˆå•ä½ï¼šç”µæœºåˆ»åº¦ï¼‰
            gain = 80.0  # è¶Šå¤§åŠ¨å¾—è¶Šå¿«ï¼›å¦‚å¤ªçŒ›å¯æ”¹æˆ 30 / 50
            delta = np.zeros_like(current)
            delta[0] = gain * off_x       # shoulder_pan
            delta[1] = -gain * off_y      # shoulder_liftï¼ˆæ‰‹å¾€ä¸Š â†’ æœºæ¢°è‡‚æŠ¬èµ·ï¼‰

            target = current + delta

            action = {
                name: float(target[i])
                for i, name in enumerate(JOINT_NAMES)
            }

            try:
                robot.send_action(action)
            except Exception as e:
                print("å‘é€åŠ¨ä½œå¤±è´¥ï¼š", e)

        centers = tracker.get_palm_centers(frame)
        if centers:
            draw_hand(frame, centers[0], is_tap, is_hold)

        draw_gesture_status(frame, is_tap, is_hold)
        draw_finger_tips(frame, tracker, is_hold)

        cv2.imshow("Gesture (q to quit)", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
    robot.disconnect()
    print("ğŸ”Œ å·²æ–­å¼€ SO100 Follower")


if __name__ == "__main__":
    main()
